/**
 * Interim Transcript Validation Test
 * 
 * Validates that interim and final transcripts are received when using fake audio samples.
 * This test uses pre-recorded audio samples (via loadAndSendAudioSample) to test transcript
 * receipt without requiring real microphone access.
 * 
 * Uses the same pattern as the passing callback-test.spec.js test.
 */

import { test, expect } from '@playwright/test';
import { loadAndSendAudioSample, waitForVADEvents } from './fixtures/audio-helpers.js';
import { getVADState } from './fixtures/vad-helpers.js';
import { pathWithQuery } from './helpers/test-helpers.mjs';
import {
  setupAudioSendingPrerequisites,
  skipIfOpenAIProxy,
  SELECTORS
} from './helpers/test-helpers.js';

test.describe('Interim Transcript Validation', () => {
  test.beforeEach(async ({ page, context }) => {
    skipIfOpenAIProxy('Interim/final transcripts are Deepgram-only; skip when using OpenAI proxy');
    // Grant microphone permissions before navigation (same pattern as callback-test.spec.js)
    await context.grantPermissions(['microphone']);
    
    // Navigate using relative path so baseURL (http/https) is applied
    await page.goto(pathWithQuery({ 'test-mode': 'true' }));
  });

  test.afterEach(async ({ page }) => {
    // Clean up: Close any open connections and clear state
    try {
      await page.evaluate(() => {
        // Close component if it exists
        if (window.deepgramRef?.current) {
          window.deepgramRef.current.stop?.();
        }
      });
      // Navigate away to ensure clean state for next test
      await page.goto('about:blank');
      await page.waitForTimeout(500); // Give time for cleanup
    } catch (error) {
      // Ignore cleanup errors - test may have already navigated away
    }
  });

  test('should receive both interim and final transcripts with fake audio', async ({ page, context }) => {
    console.log('üß™ Testing interim and final transcript receipt with fake audio...');
    
    // Capture console messages to see debug logs
    const consoleMessages = [];
    page.on('console', msg => {
      const text = msg.text();
      if (text.includes('[TRANSCRIPT-CALLBACK]') || text.includes('[TEST-CAPTURE]')) {
        consoleMessages.push(text);
        console.log('üì¢ Browser console:', text);
      }
    });
    
    // Use setupAudioSendingPrerequisites helper for audio-sending tests (same pattern as passing test)
    // This handles: permissions, component ready, mic click, connection, settings applied, settings delay
    await setupAudioSendingPrerequisites(page, context, {
      componentReadyTimeout: 5000,
      connectionTimeout: 10000,
      settingsTimeout: 10000,
      settingsProcessingDelay: 600
    });
    
    console.log('‚úÖ Connection established and settings applied');
    
    // Use streaming audio to simulate real-time audio input
    // This is critical for receiving interim transcripts - sending all at once
    // causes the API to process quickly and only send the final transcript
    // Use pre-recorded human speech (non-TTS) for more realistic interim transcript generation
    // The helper automatically handles WAV vs JSON format detection and streaming
    const sampleName = 'shopping-concierge-question';
    console.log(`üé§ Loading and streaming pre-recorded audio sample (human speech): ${sampleName}...`);
    
    await loadAndSendAudioSample(page, sampleName);
    
    console.log(`‚úÖ Audio sample streamed: ${sampleName}`);
    
    // Wait for transcript element to be visible first (same pattern as passing test)
    await page.waitForSelector(SELECTORS.transcription, { timeout: 5000 });
    
    // Wait for transcript to appear in the UI with actual content
    await page.waitForFunction((selector) => {
      const transcriptElement = document.querySelector(selector);
      if (!transcriptElement) return false;
      const text = transcriptElement.textContent?.trim() || '';
      // Transcript is valid if it's not empty and not the waiting message
      return text.length > 0 && text !== '(Waiting for transcript...)';
    }, SELECTORS.transcription, { timeout: 20000 });
    
    // Wait for VAD events in sequence - these indicate audio processing is complete
    // 1. UserStartedSpeaking: indicates audio input was detected
    // 2. UtteranceEnd: indicates the utterance is complete and final transcript should arrive
    
    console.log('‚è≥ Waiting for UserStartedSpeaking...');
    await page.waitForFunction((selector) => {
      const el = document.querySelector(selector);
      return el && el.textContent && el.textContent.trim() !== 'Not detected';
    }, SELECTORS.userStartedSpeaking, { timeout: 15000 });
    
    const userStartedSpeaking = await page.locator(SELECTORS.userStartedSpeaking).textContent();
    expect(userStartedSpeaking).not.toBe('Not detected');
    console.log('‚úÖ UserStartedSpeaking detected:', userStartedSpeaking);
    
    console.log('‚è≥ Waiting for UtteranceEnd...');
    await page.waitForFunction((selector) => {
      const el = document.querySelector(selector);
      return el && el.textContent && el.textContent.trim() !== 'Not detected';
    }, SELECTORS.utteranceEnd, { timeout: 15000 });
    
    const utteranceEnd = await page.locator(SELECTORS.utteranceEnd).textContent();
    expect(utteranceEnd).not.toBe('Not detected');
    console.log('‚úÖ UtteranceEnd detected:', utteranceEnd);
    
    console.log('\nüìä === VAD EVENTS ===');
    console.log('üé§ UserStartedSpeaking:', userStartedSpeaking);
    console.log('üîö UtteranceEnd:', utteranceEnd);
    
    // Wait a short buffer after UtteranceEnd to ensure all transcripts arrive
    // UtteranceEnd indicates the utterance is complete, but transcripts may continue arriving
    // For fake audio sent all at once, we need to wait longer to capture all interim transcripts
    await page.waitForTimeout(2000);
    
    // Wait for transcript history container to appear in DOM
    await page.waitForSelector('[data-testid="transcript-history"]', { timeout: 5000 });
    
    // Wait for at least one transcript entry to appear in the DOM
    await page.waitForFunction(
      () => {
        const entries = document.querySelectorAll('[data-testid^="transcript-entry-"]');
        return entries.length > 0;
      },
      { timeout: 5000 }
    );
    
    // Wait for final transcript to arrive (if we only got interim so far)
    // This ensures we capture both interim and final transcripts
    await page.waitForFunction(
      () => {
        const entries = document.querySelectorAll('[data-testid^="transcript-entry-"]');
        for (const entry of entries) {
          const isFinal = entry.getAttribute('data-is-final') === 'true';
          if (isFinal) return true;
        }
        return false;
      },
      { timeout: 3000 }
    ).catch(() => {
      // If no final transcript arrives within 3s, that's okay - we'll validate what we got
      console.log('‚ö†Ô∏è No final transcript detected within timeout, will validate what we have');
    });
    
    // CRITICAL: Wait for any additional interim transcripts that may still be arriving
    // Interim transcripts can arrive after UtteranceEnd, especially with fake audio
    // Wait for the transcript count to stabilize (no new transcripts for 500ms)
    let previousCount = 0;
    let stableCount = 0;
    for (let i = 0; i < 10; i++) {
      await page.waitForTimeout(500);
      const currentCount = await page.evaluate(() => {
        return document.querySelectorAll('[data-testid^="transcript-entry-"]').length;
      });
      if (currentCount === previousCount) {
        stableCount++;
        if (stableCount >= 2) {
          console.log(`‚úÖ Transcript count stabilized at ${currentCount} after ${(i + 1) * 500}ms`);
          break;
        }
      } else {
        stableCount = 0;
        console.log(`üìä Transcript count changed: ${previousCount} -> ${currentCount}`);
      }
      previousCount = currentCount;
    }
    
    // Capture transcripts from the DOM (replaces window.__testTranscripts)
    const capturedTranscripts = await page.evaluate(() => {
      const entries = Array.from(document.querySelectorAll('[data-testid^="transcript-entry-"]'));
      const transcripts = entries.map((entry, index) => {
        const textEl = entry.querySelector(`[data-testid="transcript-text-${index}"]`);
        const text = textEl?.textContent?.trim() || '';
        const isFinal = entry.getAttribute('data-is-final') === 'true';
        const speechFinal = entry.getAttribute('data-speech-final') === 'true';
        const timestamp = parseInt(entry.getAttribute('data-timestamp') || '0', 10);
        
        return {
          text,
          is_final: isFinal,
          speech_final: speechFinal,
          timestamp
        };
      });
      
      // Log for debugging
      console.log('[TEST] Transcripts from DOM:', JSON.stringify(transcripts, null, 2));
      
      return transcripts;
    });
    
    console.log('\nüìä === TRANSCRIPT VALIDATION ===');
    console.log('üìù Total transcripts captured:', capturedTranscripts.length);
    console.log('üìã Raw transcripts array:', JSON.stringify(capturedTranscripts, null, 2));
    console.log('üì¢ Captured console messages:', consoleMessages.length, 'messages');
    if (consoleMessages.length > 0) {
      console.log('üì¢ First few console messages:', consoleMessages.slice(0, 10));
    }
    
    // CRITICAL ASSERTION: Must have received at least one transcript
    expect(capturedTranscripts.length).toBeGreaterThan(0);
    
    // Separate interim and final transcripts
    const finalTranscripts = capturedTranscripts.filter(t => t.is_final === true);
    const interimTranscripts = capturedTranscripts.filter(t => t.is_final === false);
    
    console.log('üìù Final transcripts:', finalTranscripts.length);
    console.log('üìù Interim transcripts:', interimTranscripts.length);
    
    // Identify the complete final transcript
    // Deepgram sends word-by-word final transcripts, but the complete final transcript
    // is identified by speech_final=true (or the last final transcript if speech_final is not set)
    const speechFinalTranscripts = finalTranscripts.filter(t => t.speech_final === true);
    let completeFinalTranscript = null;
    
    if (speechFinalTranscripts.length > 0) {
      // Use the transcript with speech_final=true (Deepgram's indicator of complete utterance)
      completeFinalTranscript = speechFinalTranscripts[speechFinalTranscripts.length - 1];
      console.log('‚úÖ Complete final transcript identified by speech_final=true');
    } else if (finalTranscripts.length > 0) {
      // Fallback: use the last final transcript (may be word-by-word, but it's the most complete)
      completeFinalTranscript = finalTranscripts[finalTranscripts.length - 1];
      console.log('‚ö†Ô∏è No speech_final=true found, using last final transcript as complete');
    }
    
    // Report on the complete final transcript
    if (completeFinalTranscript) {
      console.log('\nüìã === COMPLETE FINAL TRANSCRIPT ===');
      console.log(`üìù Text: "${completeFinalTranscript.text}"`);
      console.log(`üìä Properties:`, {
        is_final: completeFinalTranscript.is_final,
        speech_final: completeFinalTranscript.speech_final,
        timestamp: completeFinalTranscript.timestamp,
        length: completeFinalTranscript.text.length
      });
      
      // Show all final transcripts for comparison
      console.log('\nüìã === ALL FINAL TRANSCRIPTS (for comparison) ===');
      finalTranscripts.forEach((t, idx) => {
        const isComplete = t === completeFinalTranscript;
        console.log(`${idx + 1}. ${isComplete ? '‚òÖ' : ' '} "${t.text}" (is_final: ${t.is_final}, speech_final: ${t.speech_final})`);
      });
    } else {
      console.log('‚ö†Ô∏è No complete final transcript found');
    }
    
    // Debug: Show what is_final values we actually received
    if (capturedTranscripts.length > 0) {
      console.log('\nüîç === ALL TRANSCRIPTS SUMMARY ===');
      console.log('is_final values received:', capturedTranscripts.map(t => ({
        text: t.text.substring(0, 30),
        is_final: t.is_final,
        speech_final: t.speech_final,
        is_final_type: typeof t.is_final,
        timestamp: t.timestamp
      })));
    }
    
    // NOTE: With real-time streaming (chunks at calculated intervals), interim transcripts are reliably generated.
    // This test uses WAV file streaming which consistently produces interim transcripts.
    // Reference: vad-transcript-analysis.spec.js demonstrates the working pattern.
    if (interimTranscripts.length === 0) {
      console.log('‚ö†Ô∏è [NOTE] No interim transcripts received - this may indicate streaming issues');
      console.log('   Expected: Real-time streaming should produce interim transcripts');
      console.log('   Check: Audio streaming implementation and chunk intervals');
      console.log('   Reference: See working test in vad-transcript-analysis.spec.js for streaming pattern');
      console.log('   Test will continue to validate final transcript handling...');
    } else {
      console.log('‚úÖ Interim transcripts validated:', interimTranscripts.length);
      // If we do get interim transcripts, verify they arrived before final transcripts
      if (finalTranscripts.length > 0) {
        const firstInterimTime = Math.min(...interimTranscripts.map(t => t.timestamp || 0));
        const firstFinalTime = Math.min(...finalTranscripts.map(t => t.timestamp || 0));
        expect(firstInterimTime).toBeLessThan(firstFinalTime);
        console.log('‚úÖ Interim transcripts arrived before final transcripts (as expected)');
      }
    }
    
    // The test validates that:
    // 1. Transcripts are correctly received and stored in the DOM
    // 2. Final transcripts are correctly identified
    // 3. The component's transcript handling works correctly
    // 4. Interim transcripts are generated when using real-time streaming approach
    
    // CRITICAL ASSERTION: Must have received at least one final transcript
    expect(finalTranscripts.length).toBeGreaterThan(0);
    console.log('‚úÖ Final transcripts validated:', finalTranscripts.length);
    
    // Validate that transcripts contain actual text
    capturedTranscripts.forEach((transcript) => {
      expect(transcript.text?.length).toBeGreaterThan(0);
    });
    console.log('‚úÖ All transcripts contain valid text');
    
    // Verify transcript appears in UI (same assertion as passing test)
    const transcriptText = await page.locator(SELECTORS.transcription).textContent();
    expect(transcriptText).toBeTruthy();
    expect(transcriptText?.trim()).not.toBe('');
    expect(transcriptText).not.toBe('(Waiting for transcript...)');
    
    console.log('\n‚úÖ All transcript validations passed!');
  });
});

